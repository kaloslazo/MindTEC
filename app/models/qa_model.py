import logging
import uuid
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_qdrant import Qdrant
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from app.config import config
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

PROMPT_TEMPLATE = """
Eres un asistente virtual para estudiantes de la Universidad de Ingenier√≠a y Tecnolog√≠a (UTEC). Tu tarea es proporcionar informaci√≥n precisa y relevante basada en el contenido de los s√≠labos de los cursos, promociones, actividades deportivas y ofrecer ayuda con materiales y t√©cnicas de estudio. Para mejorar la claridad y efectividad de las respuestas, sigue estas directrices estrictamente:

Formato y estilo:
* No uses triple asterisco (*).
* Usa un solo asterisco (*) para destacar informaci√≥n importante, como *nota importante.
* Usa guion bajo (_) para resaltar t√©rminos t√©cnicos o conceptos clave, ejemplo: _algoritmo.
* Usa "comillas" para citas textuales, ejemplo: "texto citado".
* Usa un solo emoji al final de la respuesta, cuando sea apropiado.
* Usa listas con guiones (-) para enumerar elementos, prohibido usar ** o * en guiones.
* Para t√≠tulos o subt√≠tulos, usa un solo asterisco (*) al inicio de la l√≠nea, sin asterisco al final.

Contexto:
{context}

Pregunta del estudiante: {question}

Instrucciones:
1. Identifica el tipo de informaci√≥n solicitada (s√≠labo, promoci√≥n, actividad deportiva, materiales de estudio o t√©cnicas de estudio).
2. Si el nombre del curso no est√° especificado, intenta inferirlo del contexto o de interacciones previas. Si no es posible, pregunta al estudiante para que aclare.
3. Extrae y utiliza informaci√≥n del contexto proporcionado para formular la respuesta.
4. Para s√≠labos:
   - Si se pregunta por referencias bibliogr√°ficas, menci√≥nalas directamente.
   - Incluye detalles relevantes como cr√©ditos, modalidad y otros datos importantes si la pregunta lo requiere.
5. Para promociones:
   - Si la pregunta es general, proporciona una lista concisa de categor√≠as y nombres de establecimientos, sin descripciones adicionales.
   - Usa el formato: "Categor√≠a: Nombre1, Nombre2".
   - Limita la respuesta a un m√°ximo de 5 categor√≠as y 3 nombres por categor√≠a.
   - Al final, pregunta si el usuario desea m√°s detalles sobre alguna promoci√≥n.
6. Para actividades deportivas:
   - Proporciona la informaci√≥n relevante sobre las actividades disponibles y ofrece preguntar al estudiante si necesita detalles adicionales.
7. Para materiales y t√©cnicas de estudio:
   - Ofrece recomendaciones de materiales de estudio y t√©cnicas de estudio adecuadas al tema o curso en cuesti√≥n.
   - Proporciona consejos pr√°cticos que puedan ayudar al estudiante en su aprendizaje.
8. Si la informaci√≥n solicitada no est√° disponible, responde: "Lo siento, no tengo informaci√≥n espec√≠fica sobre eso."
9. Evita inferir informaci√≥n que no est√© expl√≠citamente presente en el contexto.
10. Aseg√∫rate de que la respuesta sea clara, organizada y √∫til para el estudiante.
11. Evita numeraci√≥n con #.
12. Divide la respuesta en secciones progresivas y espec√≠ficas. Pregunta al estudiante si necesita m√°s detalles despu√©s de cada secci√≥n para estructurar la conversaci√≥n de manera interactiva.

Respuesta basada en la informaci√≥n proporcionada:
"""

class QAModel:
    def __init__(self, texts):
        logger.info(f"QAModel inicializado con {len(texts)} documentos")
        prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "question"])

        self.collection_name = config.QDRANT_COLLECTION_NAME
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
        self.qdrant_client = QdrantClient(url=config.QDRANT_URL, api_key=config.QDRANT_API_KEY)

        self.qdrant = Qdrant(
            client=self.qdrant_client,
            collection_name=self.collection_name,
            embeddings=self.embeddings,
        )

        self.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, max_tokens=300)
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.qdrant.as_retriever(search_kwargs={"k": 3}),
            combine_docs_chain_kwargs={"prompt": prompt},
            return_source_documents=True
        )

        self.clear_collection()
        self.load_documents(texts)

    def clear_collection(self):
        logger.info(f"Limpiando la colecci√≥n {self.collection_name}")
        try:
            self.qdrant_client.delete_collection(self.collection_name)
            logger.info(f"Colecci√≥n {self.collection_name} eliminada")
        except Exception as e:
            logger.warning(f"Error al eliminar la colecci√≥n: {e}")
        
        self.create_collection_if_not_exists()

    def create_collection_if_not_exists(self):
        collections = self.qdrant_client.get_collections().collections
        if not any(collection.name == self.collection_name for collection in collections):
            logger.info(f"Creando nueva colecci√≥n: {self.collection_name}")
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=768, distance=Distance.COSINE),
            )
        else:
            logger.info(f"La colecci√≥n {self.collection_name} ya existe")

    def split_content(self, content, max_length=500):
        sections = []
        current_section = ""
        for line in content.split('\n'):
            if len(current_section) + len(line) > max_length:
                sections.append(current_section.strip())
                current_section = line
            else:
                current_section += "\n" + line
        if current_section:
            sections.append(current_section.strip())
        return sections

    def load_documents(self, texts):
        logger.info(f"Cargando {len(texts)} documentos en Qdrant")
        points = []
        for text in texts:
            content = text.page_content
            vector = self.embeddings.embed_query(content)
            point = PointStruct(
                id=str(uuid.uuid4()),  # Generamos un UUID √∫nico para cada punto
                payload={
                    'text': content, 
                    'metadata': text.metadata,
                    'doc_type': text.metadata.get('type', 'unknown')
                },
                vector=vector
            )
            points.append(point)

        try:
            operation_info = self.qdrant_client.upsert(
                collection_name=self.collection_name,
                points=points
            )
            logger.info(f"Operaci√≥n de carga completada. Info: {operation_info}")
        except Exception as e:
            logger.error(f"Error al cargar documentos en Qdrant: {e}")
            raise

        logger.info("Documentos cargados exitosamente en Qdrant")

    def getAnswer(self, question):
        logger.info(f"Procesando pregunta: {question}")
        if len(question.split()) < 3:
            return "ü§î Por favor, proporciona m√°s detalles para poder ayudarte mejor."
        try:
            logger.debug("Iniciando b√∫squeda en Qdrant")
        
            if "promociones" in question.lower() and "universidad" in question.lower():
                query_filter = {"must": [{"key": "doc_type", "match": {"value": "promo"}}]}
            elif any(word in question.lower() for word in ["deporte", "cancha", "reserva"]):
                query_filter = {"must": [{"key": "doc_type", "match": {"value": "deporte"}}]}
            else:
                query_filter = None

            query_vector = self.embeddings.embed_query(question)
            search_results = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                query_filter=query_filter,
                limit=5
            )
        
            full_context = []
            for i, result in enumerate(search_results):
                logger.debug(f"Documento {i+1}:")
                logger.debug(f"ID: {result.id}, Score: {result.score}")
                logger.debug(f"Tipo: {result.payload.get('doc_type', 'unknown')}")
                logger.debug(f"Contenido: {result.payload['text'][:200]}...")
                full_context.append(f"Documento {i+1} ({result.payload.get('doc_type', 'unknown')}):\n{result.payload['text']}")
        
            context = "\n\n".join(full_context)
            logger.debug(f"Contexto completo pasado al modelo:\n{context}")
        
            prompt = self.qa_chain.combine_docs_chain.llm_chain.prompt.format(
                context=context,
                question=question
            )
            response = self.qa_chain.combine_docs_chain.llm_chain.llm.predict(prompt)
        
            logger.debug(f"Respuesta generada: {response}")
            return response
        except Exception as e:
            logger.error(f"Error al procesar la pregunta: {str(e)}", exc_info=True)
            return "üôÅ Lo siento, tuve un peque√±o problema al procesar tu pregunta. ¬øPodr√≠as intentar reformularla?"

    def test_retrieval(self, query):
        logger.info(f"Probando recuperaci√≥n para la consulta: {query}")
        query_vector = self.embeddings.embed_query(query)
        search_result = self.qdrant_client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=3
        )
        logger.info("Resultados de b√∫squeda directa en Qdrant:")
        for result in search_result:
            logger.info(f"ID: {result.id}, Score: {result.score}")
            logger.info(f"Contenido: {result.payload['text'][:200]}...")

        # Probar la cadena completa
        qa_result = self.getAnswer(query)
        logger.info(f"Respuesta del modelo: {qa_result}")
